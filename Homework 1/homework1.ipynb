{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58425a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta0: 3.685681355016952\n",
      "Theta1: 0.983353918106614\n",
      "Mean Squared Error: 1.5522086622355378\n"
     ]
    }
   ],
   "source": [
    "# Task 1: simple predictor that estimates rating from review length\n",
    "\n",
    "\"\"\"\n",
    "star rating ≃ θ0 + θ1 × [review length in characters].\n",
    "scale the feature to be between 0 and 1 by dividing by the maximum review length in the dataset. \n",
    "report the values θ0 and θ1, and the Mean Squared Error of your predictor (on the entire dataset).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open(\"fantasy_10000.json.gz\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(json.loads(l))\n",
    "\n",
    "# Extract review lengths and ratings\n",
    "review_lengths = [len(d['review_text']) for d in dataset]\n",
    "ratings = [d['rating'] for d in dataset]\n",
    "\n",
    "# Scale review lengths between 0 and 1\n",
    "max_length = max(review_lengths)\n",
    "scaled_lengths = [length / max_length for length in review_lengths]\n",
    "\n",
    "# Create a feature matrix\n",
    "X = np.array(scaled_lengths).reshape(-1, 1)\n",
    "y = np.array(ratings)\n",
    "\n",
    "# Fit a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get the coefficients\n",
    "theta0 = model.intercept_\n",
    "theta1 = model.coef_[0]\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "print(f\"Theta0: {theta0}\")\n",
    "print(f\"Theta1: {theta1}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfddcc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector for the first example: [1.0, 0.14581294561722355, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Feature vector for the second example: [1.0, 0.10631902698168601, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Task 2: include (in addition to the scaled length) features based on the time of the review\n",
    "\n",
    "\"\"\"\n",
    "include features based on the time of the review.\n",
    "use a one-hot encoding for the weekday and month.\n",
    "write down feature vectors for the first two examples.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "import dateutil.parser\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open(\"fantasy_10000.json.gz\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(json.loads(l))\n",
    "    \n",
    "# Extract review lengths, ratings and days\n",
    "review_lengths = np.array([len(d['review_text']) for d in dataset]).reshape(-1,1)\n",
    "ratings = np.array([d['rating'] for d in dataset])\n",
    "dates = np.array([d['date_added'] for d in dataset])\n",
    "\n",
    "# Scale the review_lengths to be between 0 and 1\n",
    "max_length = max(review_lengths)\n",
    "scaled_lengths = review_lengths / max_length\n",
    "\n",
    "# Extract weekday and month from the date strings\n",
    "weekdays = [dateutil.parser.parse(d).weekday() for d in dates]\n",
    "months = [dateutil.parser.parse(d).month for d in dates]\n",
    "\n",
    "# One-hot encoding for weekdays and months, dropping the first dimension for both\n",
    "encoder = OneHotEncoder(categories=[list(range(7)), list(range(1, 13))], drop=[0, 1], sparse=False)\n",
    "encoded_features = encoder.fit_transform(np.array([weekdays, months]).T)\n",
    "\n",
    "# Create an offset term (a column of ones)\n",
    "offsets = np.ones((len(review_lengths), 1))\n",
    "\n",
    "# Combine intercept, scaled_lengths, and encoded_features\n",
    "X = np.hstack((offsets, scaled_lengths, encoded_features))\n",
    "\n",
    "# Feature vectors for the first two examples\n",
    "print(\"Feature vector for the first example:\", list(X[0]))\n",
    "print(\"Feature vector for the second example:\", list(X[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d17623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Use one-hot encoding for the weekday and month): 1.5466315498487564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/vy1h_x1105q5pl_wdj84g_140000gn/T/ipykernel_49659/3057939873.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  feature_vector = np.array(([scaled_review_length, weekday, month]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Use the weekday and month Directly): 1.5516353711453328\n"
     ]
    }
   ],
   "source": [
    "# Task 3: use the weekday and month values directly\n",
    "\n",
    "\"\"\"\n",
    "use the weekday and month values directly as features.\n",
    "use the one-hot encoding from Question 2.\n",
    "report the MSE of each.\n",
    "\"\"\"\n",
    "\n",
    "########################## Use Weekday and Month as One-Hot ##########################\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import dateutil.parser\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y = np.array(ratings)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(f\"Mean Squared Error (Use one-hot encoding for the weekday and month): {mse}\")\n",
    "\n",
    "########################## Use Weekday and Month Directly ##########################\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import dateutil.parser\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open(\"fantasy_10000.json.gz\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(json.loads(l))\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []  # Feature vectors\n",
    "ratings = []  # Ratings\n",
    "\n",
    "# Iterate through the dataset and extract features\n",
    "for d in dataset:\n",
    "    # Parse the review date\n",
    "    t = dateutil.parser.parse(d['date_added'])\n",
    "    \n",
    "    # Extract weekday and month\n",
    "    weekday = t.weekday()\n",
    "    month = t.month\n",
    "    \n",
    "    # Calculate the length of the review text and scale it\n",
    "    review_length = len(d['review_text'])\n",
    "    scaled_review_length = review_length / max_length\n",
    "    \n",
    "    # Create the feature vector by combining all features\n",
    "    feature_vector = np.array(([scaled_review_length, weekday, month]))\n",
    "    \n",
    "    # Append the feature vector and rating to the lists\n",
    "    features.append(feature_vector)\n",
    "    ratings.append(d['rating'])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(ratings)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(f\"Mean Squared Error (Use the weekday and month Directly): {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2645543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Use the weekday and month Directly) on the test set: 1.6264453676167938\n",
      "Mean Squared Error (Use the weekday and month Directly) on the test set: 1.6282919476176059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/vy1h_x1105q5pl_wdj84g_140000gn/T/ipykernel_49659/657933453.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  feature_vector = np.array(([scaled_review_length, weekday, month]))\n"
     ]
    }
   ],
   "source": [
    "# Task 4: split the data into 50%/50% train/test\n",
    "\n",
    "\"\"\"\n",
    "split the data into 50%/50% train/test fractions.\n",
    "report the MSE of the two models on the test set.\n",
    "\"\"\"\n",
    "\n",
    "########################## Use Weekday and Month as One-Hot ##########################\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "import dateutil.parser\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open(\"fantasy_10000.json.gz\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(json.loads(l))\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(dataset)\n",
    "    \n",
    "# Extract review lengths, ratings and days\n",
    "review_lengths = np.array([len(d['review_text']) for d in dataset]).reshape(-1,1)\n",
    "ratings = np.array([d['rating'] for d in dataset])\n",
    "dates = np.array([d['date_added'] for d in dataset])\n",
    "\n",
    "# Scale the review_lengths to be between 0 and 1\n",
    "max_length = max(review_lengths)\n",
    "scaled_lengths = review_lengths / max_length\n",
    "\n",
    "# Extract weekday and month from the date strings\n",
    "weekdays = [dateutil.parser.parse(d).weekday() for d in dates]\n",
    "months = [dateutil.parser.parse(d).month for d in dates]\n",
    "\n",
    "# One-hot encoding for weekdays and months, dropping the first dimension for both\n",
    "encoder = OneHotEncoder(categories=[list(range(7)), list(range(1, 13))], drop=[0, 1], sparse=False)\n",
    "encoded_features = encoder.fit_transform(np.array([weekdays, months]).T)\n",
    "\n",
    "# Create an offset term (a column of ones)\n",
    "offsets = np.ones((len(review_lengths), 1))\n",
    "\n",
    "# Combine intercept, scaled_lengths, and encoded_features\n",
    "X = np.hstack((offsets, scaled_lengths, encoded_features))\n",
    "y = np.array(ratings)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets (50% each)\n",
    "X_train, X_test = X[:len(X)//2], X[len(X)//2:]\n",
    "y_train, y_test = y[:len(y)//2], y[len(y)//2:]\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (Use the weekday and month Directly) on the test set: {mse}\")\n",
    "\n",
    "########################## Use Weekday and Month Directly ##########################\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import dateutil.parser\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open(\"fantasy_10000.json.gz\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(json.loads(l))\n",
    "    \n",
    "random.seed(0)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []  # Feature vectors\n",
    "ratings = []  # Ratings\n",
    "\n",
    "# Iterate through the dataset and extract features\n",
    "for d in dataset:\n",
    "    # Parse the review date\n",
    "    t = dateutil.parser.parse(d['date_added'])\n",
    "    \n",
    "    # Extract weekday and month\n",
    "    weekday = t.weekday()\n",
    "    month = t.month\n",
    "    \n",
    "    # Calculate the length of the review text and scale it\n",
    "    review_length = len(d['review_text'])\n",
    "    scaled_review_length = review_length / max_length\n",
    "    \n",
    "    # Create the feature vector by combining all features\n",
    "    feature_vector = np.array(([scaled_review_length, weekday, month]))\n",
    "    \n",
    "    # Append the feature vector and rating to the lists\n",
    "    features.append(feature_vector)\n",
    "    ratings.append(d['rating'])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(ratings)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets (50% each)\n",
    "X_train, X_test = X[:len(X)//2], X[len(X)//2:]\n",
    "y_train, y_test = y[:len(y)//2], y[len(y)//2:]\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (Use the weekday and month Directly) on the test set: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11357391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 14201\n",
      "True Negatives: 10503\n",
      "False Positives: 5885\n",
      "False Negatives: 19411\n",
      "Balanced Error Rate (BER): 0.4683031525957275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuke/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Task 5: fit a logistic regressor\n",
    "\n",
    "\"\"\"\n",
    "fit a logistic regressor that estimates the binarized score from review length.\n",
    "use the class weight=’balanced’ option, report the number of True Positives, \n",
    "True Negatives, False Positives, False Negatives, and the Balanced Error Rate of the classifier.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "# Load the data from \"beer_50000.json\"\n",
    "f = open(\"beer_50000.json\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(eval(l))\n",
    "\n",
    "# Create a label vector (1 for positive, 0 for negative) based on review scores\n",
    "labels = np.array([1 if data['review/overall'] >= 4 else 0 for data in dataset]).reshape(-1,1)\n",
    "\n",
    "# Extract the review lengths\n",
    "review_lengths = np.array([len(data['review/text']) for data in dataset]).reshape(-1,1)\n",
    "\n",
    "# Fit a logistic regression model with class_weight='balanced'\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(review_lengths, labels)\n",
    "\n",
    "# Predict the labels on the dataset\n",
    "pred = model.predict(review_lengths)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(labels, pred).ravel()\n",
    "\n",
    "# Calculate the Balanced Error Rate (BER)\n",
    "ber = 1 - balanced_accuracy_score(labels, pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"True Positives:\", tp)\n",
    "print(\"True Negatives:\", tn)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"False Negatives:\", fn)\n",
    "print(\"Balanced Error Rate (BER):\", ber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0716638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 1.0\n",
      "Precision@100: 0.75\n",
      "Precision@1000: 0.71\n",
      "Precision@10000: 0.7146\n"
     ]
    }
   ],
   "source": [
    "# Task 6: compute the precision of classifier\n",
    "\n",
    "\"\"\"\n",
    "compute the precision@K of classifier for K ∈ {1, 100, 1000, 10000}.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Load the data from \"beer_50000.json\"\n",
    "f = open(\"beer_50000.json\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(eval(l))\n",
    "\n",
    "# Construct the label vector\n",
    "y = np.array([d['review/overall'] >= 4 for d in dataset])\n",
    "\n",
    "# Extract the review length\n",
    "X = np.array([len(d['review/text']) for d in dataset]).reshape(-1, 1)\n",
    "\n",
    "# Fit a logistic regressor\n",
    "log_reg = LogisticRegression(class_weight='balanced')\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "# Compute precision at K\n",
    "K_values = [1, 100, 1000, 10000]\n",
    "for k in K_values:\n",
    "    confidence_scores = log_reg.decision_function(X)\n",
    "    predicted_ranking = np.argsort(confidence_scores)[::-1][:k]\n",
    "    y_pred_at_k = np.zeros(len(y))\n",
    "    y_pred_at_k[predicted_ranking] = 1\n",
    "    precision_at_k = precision_score(y, y_pred_at_k)\n",
    "    print(f\"Precision@{k}: {precision_at_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94242b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Error Rate (BER): 0.16405903360796215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuke/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Task 7: improve the classifier\n",
    "\n",
    "\"\"\"\n",
    "reduce the balanced error rate by incorporating additional features from the data.\n",
    "describe your improvement (as a string) and report the BER of your new predictor.\n",
    "\"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the data from \"beer_50000.json\"\n",
    "f = open(\"beer_50000.json\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(eval(l))\n",
    "\n",
    "# Create a label vector (1 for positive, 0 for negative) based on review scores\n",
    "labels = [1 if data['review/overall'] >= 4 else 0 for data in dataset]\n",
    "\n",
    "# Extract additional features: 'beer/style', ratings, and review text\n",
    "beer_styles = [data['beer/style'] for data in dataset]\n",
    "ratings = np.array([[data['review/appearance'], data['review/aroma'], data['review/palate'], data['review/taste']] for data in dataset])\n",
    "review_text = [data['review/text'] for data in dataset]\n",
    "\n",
    "# Encode beer styles using one-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "beer_styles_encoded = encoder.fit_transform(np.array(beer_styles).reshape(-1, 1))\n",
    "\n",
    "# Scale ratings using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "ratings_scaled = scaler.fit_transform(ratings)\n",
    "\n",
    "# Vectorize the review text using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust the number of features as needed\n",
    "review_text_tfidf = tfidf_vectorizer.fit_transform(review_text)\n",
    "\n",
    "# Combine all the features into one feature matrix\n",
    "X = hstack((beer_styles_encoded, ratings_scaled, review_text_tfidf))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)\n",
    "\n",
    "# Fit a logistic regression model with class_weight='balanced'\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the Balanced Error Rate (BER)\n",
    "ber = 1 - balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Balanced Error Rate (BER):\", ber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13450feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Template\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "import numpy\n",
    "import random\n",
    "import gzip\n",
    "import dateutil.parser\n",
    "import math\n",
    "\n",
    "answers = {}\n",
    "\n",
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4af77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1\n",
    "\n",
    "answers['Q1'] = [3.685681355016952, 0.983353918106614, 1.5522086622355378]\n",
    "assertFloatList(answers['Q1'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6272236",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2\n",
    "\n",
    "answers['Q2'] = [[1,0.14581294561722355,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0], \n",
    "                 [1,0.10631902698168601,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0]]\n",
    "assertFloatList(answers['Q2'][0], 19)\n",
    "assertFloatList(answers['Q2'][1], 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f6fe384",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3\n",
    "\n",
    "answers['Q3'] = [1.5467978637695312, 1.5516353711453328]\n",
    "assertFloatList(answers['Q3'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03f1a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 4\n",
    "\n",
    "answers['Q4'] = [1.6264453676167938, 1.6282919476176059]\n",
    "assertFloatList(answers['Q4'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32d61c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 5\n",
    "\n",
    "answers['Q5'] = [14201, 10503, 5885, 19411, 0.4683031525957275]\n",
    "assertFloatList(answers['Q5'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "361d7a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 6\n",
    "\n",
    "answers['Q6'] = [1.0,0.75,0.71,0.7146]\n",
    "assertFloatList(answers['Q6'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2dfdcbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7\n",
    "its_test_BER = 0.16405903360796215\n",
    "answers['Q7'] = [\"Add beer styles, ratings, features from text\", its_test_BER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c7d5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw1.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
