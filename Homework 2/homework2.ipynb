{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4519f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set - Accuracy: 0.84904, BER: 0.16130237168160533\n",
      "Test Set - Accuracy: 0.85096, BER: 0.1607838024608832\n"
     ]
    }
   ],
   "source": [
    "# Task 1: use the style of the beer to predict its ABV\n",
    "\n",
    "\"\"\"\n",
    "construct a one-hot encoding of the beer style, for those categories that appear in more than 1,000 reviews.\n",
    "train a logistic regressor using this one-hot encoding to predict whether beers have an ABV greater than 7 percent. \n",
    "train the classifier on the training set.\n",
    "report its performance in terms of the accuracy and Balanced Error Rate (BER) on the validation and test sets.\n",
    "use a regularization constant of C = 10.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Function to parse the data from the file\n",
    "def parseData(fname):\n",
    "    for l in open(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "# Loading and shuffling the data\n",
    "data = list(parseData(\"beer_50000.json\"))\n",
    "random.seed(0)\n",
    "random.shuffle(data)\n",
    "\n",
    "# Splitting the data into train, validation, and test sets\n",
    "dataTrain = data[:25000]\n",
    "dataValid = data[25000:37500]\n",
    "dataTest = data[37500:]\n",
    "\n",
    "# Creating target labels for the data sets\n",
    "yTrain = [d['beer/ABV'] > 7 for d in dataTrain]\n",
    "yValid = [d['beer/ABV'] > 7 for d in dataValid]\n",
    "yTest = [d['beer/ABV'] > 7 for d in dataTest]\n",
    "\n",
    "# Calculating category counts for beer styles\n",
    "categoryCounts = defaultdict(int)\n",
    "for d in data:\n",
    "    categoryCounts[d['beer/style']] += 1\n",
    "# Filtering categories based on counts\n",
    "categories = [c for c in categoryCounts if categoryCounts[c] > 1000]\n",
    "# Creating a mapping of categories to feature indices\n",
    "catID = dict(zip(list(categories), range(len(categories))))\n",
    "\n",
    "# One-hot encoding function\n",
    "def feature(datum):\n",
    "    feat = [0] * len(categories)\n",
    "    if datum['beer/style'] in categories:\n",
    "        feat[catID[datum['beer/style']]] = 1\n",
    "    return feat\n",
    "\n",
    "# Applying one-hot encoding to the data sets\n",
    "XTrain = [feature(d) for d in dataTrain]\n",
    "XValid = [feature(d) for d in dataValid]\n",
    "XTest = [feature(d) for d in dataTest]\n",
    "\n",
    "# Logistic Regression model training\n",
    "logreg = linear_model.LogisticRegression(C=10, class_weight='balanced')\n",
    "logreg.fit(XTrain, yTrain)\n",
    "\n",
    "# Making predictions on the validation and test sets\n",
    "yValidPred = logreg.predict(XValid)\n",
    "yTestPred = logreg.predict(XTest)\n",
    "\n",
    "# Computing performance metrics\n",
    "valid_accuracy = accuracy_score(yValid, yValidPred)\n",
    "valid_ber = 1 - balanced_accuracy_score(yValid, yValidPred)\n",
    "\n",
    "test_accuracy = accuracy_score(yTest, yTestPred)\n",
    "test_ber = 1 - balanced_accuracy_score(yTest, yTestPred)\n",
    "\n",
    "# Printing the performance metrics\n",
    "print(f'Validation Set - Accuracy: {valid_accuracy}, BER: {valid_ber}')\n",
    "print(f'Test Set - Accuracy: {test_accuracy}, BER: {test_ber}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eea66a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Model - Validation BER: 0.14214313781636712\n",
      "Extended Model - Test BER: 0.14301810257164882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuke/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Task 2: extend model to include more features\n",
    "\n",
    "\"\"\"\n",
    "extend the model to include a vector of five ratings and the review length (in characters). \n",
    "scale the ‘length’ feature to be between 0 and 1 by dividing by the maximum length seen during training. \n",
    "use C = 10 and report the validation and test BER of the new classifier.\n",
    "\"\"\"\n",
    "\n",
    "# Function to parse the data from the file\n",
    "def parseData(fname):\n",
    "    for l in open(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "# Loading and shuffling the data\n",
    "data = list(parseData(\"beer_50000.json\"))\n",
    "random.seed(0)\n",
    "random.shuffle(data)\n",
    "\n",
    "# Splitting the data into train, validation, and test sets\n",
    "dataTrain = data[:25000]\n",
    "dataValid = data[25000:37500]\n",
    "dataTest = data[37500:]\n",
    "\n",
    "# Creating target labels for the data sets\n",
    "yTrain = [d['beer/ABV'] > 7 for d in dataTrain]\n",
    "yValid = [d['beer/ABV'] > 7 for d in dataValid]\n",
    "yTest = [d['beer/ABV'] > 7 for d in dataTest]\n",
    "\n",
    "# Calculating category counts for beer styles\n",
    "categoryCounts = defaultdict(int)\n",
    "for d in data:\n",
    "    categoryCounts[d['beer/style']] += 1\n",
    "# Filtering categories based on counts\n",
    "categories = [c for c in categoryCounts if categoryCounts[c] > 1000]\n",
    "# Creating a mapping of categories to feature indices\n",
    "catID = dict(zip(list(categories), range(len(categories))))\n",
    "\n",
    "# Function to extract features from the data\n",
    "def feature(datum, max_length):\n",
    "    # Create one-hot encoded features for beer styles\n",
    "    feat = [0] * len(categories)\n",
    "    if datum['beer/style'] in categories:\n",
    "        feat[catID[datum['beer/style']]] = 1\n",
    "\n",
    "    # Extract the review ratings\n",
    "    ratings = [datum['review/aroma'], datum['review/appearance'], datum['review/palate'], datum['review/taste'], datum['review/overall']]\n",
    "\n",
    "    # Extract the review length\n",
    "    length = len(datum['review/text'])\n",
    "\n",
    "    # Scale the 'length' feature to be between 0 and 1\n",
    "    length_scaled = length / max_length\n",
    "\n",
    "    # Combine all features into a single list\n",
    "    feat += ratings + [length_scaled]\n",
    "    \n",
    "    return feat\n",
    "\n",
    "# Computing the maximum length of reviews in the training set\n",
    "max_length_train = max(len(datum['review/text']) for datum in dataTrain)\n",
    "\n",
    "# Applying the extended features to the data sets\n",
    "XTrain = [feature(d, max_length_train) for d in dataTrain]\n",
    "XValid = [feature(d, max_length_train) for d in dataValid]\n",
    "XTest = [feature(d, max_length_train) for d in dataTest]\n",
    "\n",
    "# Logistic Regression model training with extended features\n",
    "logreg_extended = linear_model.LogisticRegression(C=10, class_weight='balanced')\n",
    "logreg_extended.fit(XTrain, yTrain)\n",
    "\n",
    "# Making predictions on the validation and test sets using the extended model\n",
    "yValidPred_extended = logreg_extended.predict(XValid)\n",
    "yTestPred_extended = logreg_extended.predict(XTest)\n",
    "\n",
    "# Computing performance metrics for the extended model\n",
    "valid_ber_extended = 1 - balanced_accuracy_score(yValid, yValidPred_extended)\n",
    "test_ber_extended = 1 - balanced_accuracy_score(yTest, yTestPred_extended)\n",
    "\n",
    "# Printing the validation and test BER for the extended model\n",
    "print(f'Extended Model - Validation BER: {valid_ber_extended}')\n",
    "print(f'Extended Model - Test BER: {test_ber_extended}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50eb1e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuke/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuke/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/xuke/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BER for each value of C:\n",
      "C=0.001: Validation BER=0.19604476465202803\n",
      "C=0.01: Validation BER=0.14373312636542224\n",
      "C=0.1: Validation BER=0.14301529190627793\n",
      "C=1: Validation BER=0.14426634433723184\n",
      "C=10: Validation BER=0.1446190978287677\n",
      "Best C value: 0.1\n",
      "Model performance on the validation set (C=0.1): Validation BER=0.1432824729179234\n",
      "Model performance on the test set (C=0.1): Test BER=0.14544069028123996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuke/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Task 3: implement a complete regularization pipeline with the balanced classifier\n",
    "\n",
    "\"\"\"\n",
    "split your data from above in half so that you have 50%/25%/25% train/validation/test fractions. \n",
    "consider values of C in the range {0.001, 0.01, 0.1, 1, 10}. \n",
    "report the validation BER for each value of C. \n",
    "report which value of C you would ultimately select for your model, and that model’s performance on the validation and test sets.\n",
    "\"\"\"\n",
    "\n",
    "# Splitting data into 50%/25%/25% train/validation/test fractions\n",
    "dataTrain, dataOthers = train_test_split(data, test_size=0.5, random_state=0)\n",
    "dataValid, dataTest = train_test_split(dataOthers, test_size=0.5, random_state=0)\n",
    "\n",
    "# Extracting target labels for the new data splits\n",
    "yTrain = [d['beer/ABV'] > 7 for d in dataTrain]\n",
    "yValid = [d['beer/ABV'] > 7 for d in dataValid]\n",
    "yTest = [d['beer/ABV'] > 7 for d in dataTest]\n",
    "\n",
    "# Computing the maximum length of reviews in the new training set\n",
    "max_length_train = max(len(datum['review/text']) for datum in dataTrain)\n",
    "\n",
    "# Defining the range of C values to consider\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "# Dictionary to store the validation BER for each value of C\n",
    "validation_ber_dict = {}\n",
    "\n",
    "# Training and evaluating the model for each value of C\n",
    "for C in C_values:\n",
    "    # Feature extraction with extended features and scaling\n",
    "    XTrain = [feature(d, max_length_train) for d in dataTrain]\n",
    "    XValid = [feature(d, max_length_train) for d in dataValid]\n",
    "\n",
    "    # Logistic Regression model training with current C value\n",
    "    logreg_regularized = linear_model.LogisticRegression(C=C, class_weight='balanced')\n",
    "    logreg_regularized.fit(XTrain, yTrain)\n",
    "\n",
    "    # Making predictions on the validation set\n",
    "    yValidPred_regularized = logreg_regularized.predict(XValid)\n",
    "\n",
    "    # Computing and storing the validation BER for the current C value\n",
    "    validation_ber = 1 - balanced_accuracy_score(yValid, yValidPred_regularized)\n",
    "    validation_ber_dict[C] = validation_ber\n",
    "\n",
    "# Finding the value of C that gives the lowest validation BER\n",
    "best_C = min(validation_ber_dict, key=validation_ber_dict.get)\n",
    "\n",
    "# Retrain the model on the combined train and validation sets using the best C value\n",
    "XTrainFull = [feature(d, max_length_train) for d in dataTrain + dataValid]\n",
    "yTrainFull = yTrain + yValid\n",
    "logreg_best = linear_model.LogisticRegression(C=best_C, class_weight='balanced')\n",
    "logreg_best.fit(XTrainFull, yTrainFull)\n",
    "\n",
    "# Evaluate the model's performance on the validation and test sets using the best C value\n",
    "yValidPred_best = logreg_best.predict([feature(d, max_length_train) for d in dataValid])\n",
    "yTestPred_best = logreg_best.predict([feature(d, max_length_train) for d in dataTest])\n",
    "validation_ber_best = 1 - balanced_accuracy_score(yValid, yValidPred_best)\n",
    "test_ber_best = 1 - balanced_accuracy_score(yTest, yTestPred_best)\n",
    "\n",
    "# Printing the validation BER for each value of C and the best C value\n",
    "print(\"Validation BER for each value of C:\")\n",
    "for C, val_ber in validation_ber_dict.items():\n",
    "    print(f\"C={C}: Validation BER={val_ber}\")\n",
    "print(f\"Best C value: {best_C}\")\n",
    "\n",
    "# Printing the model's performance on the validation and test sets using the best C value\n",
    "print(f\"Model performance on the validation set (C={best_C}): Validation BER={validation_ber_best}\")\n",
    "print(f\"Model performance on the test set (C={best_C}): Test BER={test_ber_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1b0162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BER without Beer Style: 0.3139492057092712\n",
      "Test BER without Ratings: 0.16109632033831978\n",
      "Test BER without Length: 0.14658340274812243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuke/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Task 4: An ablation study\n",
    "\n",
    "\"\"\"\n",
    "measure the marginal benefit of various features by re-training the model with one feature ‘ablated’ at a time. \n",
    "consider each of the three features in your classifier above, and setting C = 1.\n",
    "report the test BER with only the other two features and the third deleted.\n",
    "\"\"\"\n",
    "\n",
    "# Function to parse the data from the file\n",
    "def parseData(fname):\n",
    "    for l in open(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "# Loading and shuffling the data\n",
    "data = list(parseData(\"beer_50000.json\"))\n",
    "random.seed(0)\n",
    "random.shuffle(data)\n",
    "\n",
    "# Splitting the data into train, validation, and test sets\n",
    "dataTrain = data[:25000]\n",
    "dataValid = data[25000:37500]\n",
    "dataTest = data[37500:]\n",
    "\n",
    "# Creating target labels for the data sets\n",
    "yTrain = [d['beer/ABV'] > 7 for d in dataTrain]\n",
    "yValid = [d['beer/ABV'] > 7 for d in dataValid]\n",
    "yTest = [d['beer/ABV'] > 7 for d in dataTest]\n",
    "\n",
    "# Calculating category counts for beer styles\n",
    "categoryCounts = defaultdict(int)\n",
    "for d in data:\n",
    "    categoryCounts[d['beer/style']] += 1\n",
    "# Filtering categories based on counts\n",
    "categories = [c for c in categoryCounts if categoryCounts[c] > 1000]\n",
    "# Creating a mapping of categories to feature indices\n",
    "catID = dict(zip(list(categories), range(len(categories))))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Ablation study for 'beer style' feature\n",
    "def feature_without_style(datum, max_length):\n",
    "    # Extract the review ratings\n",
    "    ratings = [datum['review/aroma'], datum['review/appearance'], datum['review/palate'], datum['review/taste'], datum['review/overall']]\n",
    "\n",
    "    # Extract the review length\n",
    "    length = len(datum['review/text'])\n",
    "\n",
    "    # Scale the 'length' feature to be between 0 and 1\n",
    "    length_scaled = length / max_length\n",
    "\n",
    "    # Combine all features into a single list\n",
    "    feat = ratings + [length_scaled]\n",
    "    \n",
    "    return feat\n",
    "\n",
    "# Ablation study for 'ratings' feature\n",
    "def feature_without_ratings(datum, max_length):\n",
    "    # Create one-hot encoded features for beer styles\n",
    "    feat = [0] * len(categories)\n",
    "    if datum['beer/style'] in categories:\n",
    "        feat[catID[datum['beer/style']]] = 1\n",
    "\n",
    "    # Extract the review length\n",
    "    length = len(datum['review/text'])\n",
    "\n",
    "    # Scale the 'length' feature to be between 0 and 1\n",
    "    length_scaled = length / max_length\n",
    "\n",
    "    # Combine all features into a single list\n",
    "    feat += [length_scaled]\n",
    "    \n",
    "    return feat\n",
    "\n",
    "# Ablation study for 'length' feature\n",
    "def feature_without_length(datum):\n",
    "    # Create one-hot encoded features for beer styles\n",
    "    feat = [0] * len(categories)\n",
    "    if datum['beer/style'] in categories:\n",
    "        feat[catID[datum['beer/style']]] = 1\n",
    "\n",
    "    # Extract the review ratings\n",
    "    ratings = [datum['review/aroma'], datum['review/appearance'], datum['review/palate'], datum['review/taste'], datum['review/overall']]\n",
    "\n",
    "    # Combine all features into a single list\n",
    "    feat += ratings\n",
    "    \n",
    "    return feat\n",
    "\n",
    "# Logistic Regression model training with only 'ratings' and 'length' features\n",
    "logreg_ratings_length = linear_model.LogisticRegression(C=1, class_weight='balanced')\n",
    "\n",
    "# Logistic Regression model training with only 'style' and 'length' features\n",
    "logreg_style_length = linear_model.LogisticRegression(C=1, class_weight='balanced')\n",
    "\n",
    "# Logistic Regression model training with only 'style' and 'ratings' features\n",
    "logreg_style_ratings = linear_model.LogisticRegression(C=1, class_weight='balanced')\n",
    "\n",
    "# Training the models after removing each feature\n",
    "logreg_ratings_length.fit([feature_without_style(d, max_length_train) for d in dataTrain], yTrain)\n",
    "logreg_style_length.fit([feature_without_ratings(d, max_length_train) for d in dataTrain], yTrain)\n",
    "logreg_style_ratings.fit([feature_without_length(d) for d in dataTrain], yTrain)\n",
    "\n",
    "# Making predictions on the test set using the models without each feature\n",
    "yTestPred_ratings_length = logreg_ratings_length.predict([feature_without_style(d, max_length_train) for d in dataTest])\n",
    "yTestPred_style_length = logreg_style_length.predict([feature_without_ratings(d, max_length_train) for d in dataTest])\n",
    "yTestPred_style_ratings = logreg_style_ratings.predict([feature_without_length(d) for d in dataTest])\n",
    "\n",
    "# Computing the test BER for each ablated feature\n",
    "test_ber_ratings_length = 1 - balanced_accuracy_score(yTest, yTestPred_ratings_length)\n",
    "test_ber_style_length = 1 - balanced_accuracy_score(yTest, yTestPred_style_length)\n",
    "test_ber_style_ratings = 1 - balanced_accuracy_score(yTest, yTestPred_style_ratings)\n",
    "\n",
    "# Printing the results of the ablation study\n",
    "print(f'Test BER without Beer Style: {test_ber_ratings_length}')\n",
    "print(f'Test BER without Ratings: {test_ber_style_length}')\n",
    "print(f'Test BER without Length: {test_ber_style_ratings}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a54921be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.015228426395939087, Item ID: B00H7NFDKA\n",
      "Jaccard Similarity: 0.014492753623188406, Item ID: B00QKVV3HC\n",
      "Jaccard Similarity: 0.014492753623188406, Item ID: B00GXRMD7W\n",
      "Jaccard Similarity: 0.014084507042253521, Item ID: B00H7ILRRI\n",
      "Jaccard Similarity: 0.014084507042253521, Item ID: B0057RUMPO\n",
      "Jaccard Similarity: 0.014084507042253521, Item ID: B000B6DTYW\n",
      "Jaccard Similarity: 0.013888888888888888, Item ID: B00L2708TI\n",
      "Jaccard Similarity: 0.013513513513513514, Item ID: B009Z1KKWI\n",
      "Jaccard Similarity: 0.013513513513513514, Item ID: B000VYINCW\n",
      "Jaccard Similarity: 0.013333333333333334, Item ID: B003F2BDZQ\n"
     ]
    }
   ],
   "source": [
    "# Task 5: 10 items have the highest Jaccard similarity compared to item ‘B00KCHRKD6’\n",
    "\n",
    "\"\"\"\n",
    "report both similarities and item IDs for the 10 most similar items.\n",
    "\"\"\"\n",
    "\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "path = \"amazon_reviews_us_Musical_Instruments_v1_00.tsv.gz\"\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "\n",
    "header = f.readline()\n",
    "header = header.strip().split('\\t')\n",
    "dataset = []\n",
    "\n",
    "pairsSeen = set()\n",
    "\n",
    "for line in f:\n",
    "    fields = line.strip().split('\\t')\n",
    "    d = dict(zip(header, fields))\n",
    "    ui = (d['customer_id'], d['product_id'])\n",
    "    if ui in pairsSeen:\n",
    "        # print(\"Skipping duplicate user/item:\", ui)\n",
    "        continue\n",
    "    pairsSeen.add(ui)\n",
    "    d['star_rating'] = int(d['star_rating'])\n",
    "    d['helpful_votes'] = int(d['helpful_votes'])\n",
    "    d['total_votes'] = int(d['total_votes'])\n",
    "    dataset.append(d)\n",
    "\n",
    "dataTrain = dataset[:int(len(dataset)*0.9)]\n",
    "dataTest = dataset[int(len(dataset)*0.9):]\n",
    "\n",
    "usersPerItem = defaultdict(set) # Maps an item to the users who rated it\n",
    "itemsPerUser = defaultdict(set) # Maps a user to the items that they rated\n",
    "itemNames = {}\n",
    "ratingDict = {} # To retrieve a rating for a specific user/item pair\n",
    "reviewsPerUser = defaultdict(list)\n",
    "\n",
    "for d in dataTrain:\n",
    "    user, item = d['customer_id'], d['product_id']\n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)\n",
    "    itemNames[item] = d['product_title']\n",
    "    ratingDict[user, item] = d['star_rating']\n",
    "    reviewsPerUser[user].append(d)\n",
    "\n",
    "userAverages = {}\n",
    "itemAverages = {}\n",
    "\n",
    "for u in itemsPerUser:\n",
    "    userRatings = [ratingDict[u, i] for i in itemsPerUser[u]]\n",
    "    userAverages[u] = sum(userRatings) / len(userRatings)\n",
    "\n",
    "for i in usersPerItem:\n",
    "    itemRatings = [ratingDict[u, i] for u in usersPerItem[i]]\n",
    "    itemAverages[i] = sum(itemRatings) / len(itemRatings)\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "def mostSimilar(i, N):\n",
    "    similarities = []\n",
    "    for i2 in itemNames:\n",
    "        if i2 != i:\n",
    "            sim = Jaccard(usersPerItem[i], usersPerItem[i2])\n",
    "            similarities.append((sim, i2))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:N]\n",
    "\n",
    "query = 'B00KCHRKD6'\n",
    "ms = mostSimilar(query, 10)\n",
    "for sim, item in ms:\n",
    "    print(f\"Jaccard Similarity: {sim}, Item ID: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f27deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on the test set: 1.7165666373341593\n"
     ]
    }
   ],
   "source": [
    "# Task 6: implement a rating prediction model based on the similarity function\n",
    "\n",
    "\"\"\"\n",
    "split the data into 90% train and 10% testing portions. \n",
    "when computing similarities return the item’s average rating if no similar items exist (i.e., if the denominator is zero).\n",
    "or the global average rating if that item hasn’t been seen before. \n",
    "all averages should be computed on the training set only. \n",
    "report the MSE of this rating prediction function on the test set when Sim(i, j) = Jaccard(i, j).\n",
    "\"\"\"\n",
    "\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the data\n",
    "path = \"amazon_reviews_us_Musical_Instruments_v1_00.tsv.gz\"\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "\n",
    "header = f.readline()\n",
    "header = header.strip().split('\\t')\n",
    "dataset = []\n",
    "\n",
    "pairsSeen = set()\n",
    "\n",
    "for line in f:\n",
    "    fields = line.strip().split('\\t')\n",
    "    d = dict(zip(header, fields))\n",
    "    ui = (d['customer_id'], d['product_id'])\n",
    "    if ui in pairsSeen:\n",
    "        # print(\"Skipping duplicate user/item:\", ui)\n",
    "        continue\n",
    "    pairsSeen.add(ui)\n",
    "    d['star_rating'] = int(d['star_rating'])\n",
    "    d['helpful_votes'] = int(d['helpful_votes'])\n",
    "    d['total_votes'] = int(d['total_votes'])\n",
    "    dataset.append(d)\n",
    "\n",
    "dataTrain = dataset[:int(len(dataset) * 0.9)]\n",
    "dataTest = dataset[int(len(dataset) * 0.9):]\n",
    "\n",
    "# Preprocess the data\n",
    "usersPerItem = defaultdict(set)  # Maps an item to the users who rated it\n",
    "itemsPerUser = defaultdict(set)  # Maps a user to the items that they rated\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "ratingDict = {}  # To retrieve a rating for a specific user/item pair\n",
    "\n",
    "for d in dataTrain:\n",
    "    usersPerItem[d['product_id']].add(d['customer_id'])\n",
    "    itemsPerUser[d['customer_id']].add(d['product_id'])\n",
    "    reviewsPerUser[d['customer_id']].append(d)\n",
    "    reviewsPerItem[d['product_id']].append(d)\n",
    "\n",
    "# Implement the Jaccard similarity function\n",
    "def Jaccard(s1, s2):\n",
    "    intersection = len(s1 & s2)\n",
    "    union = len(s1 | s2)\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "ratingMean = sum([d['star_rating'] for d in dataTrain])/len(dataTrain)\n",
    "\n",
    "# Calculate user and item averages\n",
    "usersAverages = {}\n",
    "itemsAverages = {}\n",
    "\n",
    "for user in reviewsPerUser.keys():\n",
    "    usersAverages[user]=sum(d['star_rating'] for d in reviewsPerUser[user])/len(reviewsPerUser[user])\n",
    "\n",
    "for item in reviewsPerItem.keys():\n",
    "    itemsAverages[item]=sum(d['star_rating'] for d in reviewsPerItem[item])/len(reviewsPerItem[item])\n",
    "        \n",
    "def predictRating_Jaccard(user,item):\n",
    "    if item not in usersPerItem:\n",
    "        return ratingMean\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        k = d['product_id']\n",
    "        if k != item:\n",
    "            ratings.append(d['star_rating'] - itemsAverages[k])\n",
    "            similarities.append(Jaccard(usersPerItem[item],usersPerItem[k]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return itemsAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        return itemsAverages[item]\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "labels = [d['star_rating'] for d in dataTest]\n",
    "simPredictions = [predictRating_Jaccard(d['customer_id'],d['product_id']) for d in dataTest]\n",
    "\n",
    "mse = MSE(simPredictions,labels)\n",
    "print(\"Mean Squared Error (MSE) on the test set:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf877a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on the test set with time decay: 1.716566096661067\n"
     ]
    }
   ],
   "source": [
    "# Task 7: time-weight collaborative filtering\n",
    "\n",
    "\"\"\"\n",
    "design a decay function that outperforms (in terms of the MSE) the trivial function f(tu,j) = 1.\n",
    "documente any design choices you make.\n",
    "\"\"\"\n",
    "\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Load the data\n",
    "path = \"amazon_reviews_us_Musical_Instruments_v1_00.tsv.gz\"\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "\n",
    "header = f.readline()\n",
    "header = header.strip().split('\\t')\n",
    "dataset = []\n",
    "\n",
    "pairsSeen = set()\n",
    "\n",
    "for line in f:\n",
    "    fields = line.strip().split('\\t')\n",
    "    d = dict(zip(header, fields))\n",
    "    ui = (d['customer_id'], d['product_id'])\n",
    "    if ui in pairsSeen:\n",
    "        # print(\"Skipping duplicate user/item:\", ui)\n",
    "        continue\n",
    "    pairsSeen.add(ui)\n",
    "    d['star_rating'] = int(d['star_rating'])\n",
    "    d['helpful_votes'] = int(d['helpful_votes'])\n",
    "    d['total_votes'] = int(d['total_votes'])\n",
    "    dataset.append(d)\n",
    "\n",
    "dataTrain = dataset[:int(len(dataset) * 0.9)]\n",
    "dataTest = dataset[int(len(dataset) * 0.9):]\n",
    "\n",
    "# Preprocess the data\n",
    "usersPerItem = defaultdict(set)  # Maps an item to the users who rated it\n",
    "itemsPerUser = defaultdict(set)  # Maps a user to the items that they rated\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "ratingDict = {}  # To retrieve a rating for a specific user/item pair\n",
    "\n",
    "for d in dataTrain:\n",
    "    usersPerItem[d['product_id']].add(d['customer_id'])\n",
    "    itemsPerUser[d['customer_id']].add(d['product_id'])\n",
    "    reviewsPerUser[d['customer_id']].append(d)\n",
    "    reviewsPerItem[d['product_id']].append(d)\n",
    "\n",
    "# Implement the Jaccard similarity function\n",
    "def Jaccard(s1, s2):\n",
    "    intersection = len(s1 & s2)\n",
    "    union = len(s1 | s2)\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "ratingMean = sum([d['star_rating'] for d in dataTrain]) / len(dataTrain)\n",
    "\n",
    "# Calculate user and item averages\n",
    "usersAverages = {}\n",
    "itemsAverages = {}\n",
    "\n",
    "for user in reviewsPerUser.keys():\n",
    "    usersAverages[user] = sum(d['star_rating'] for d in reviewsPerUser[user]) / len(reviewsPerUser[user])\n",
    "\n",
    "for item in reviewsPerItem.keys():\n",
    "    itemsAverages[item] = sum(d['star_rating'] for d in reviewsPerItem[item]) / len(reviewsPerItem[item])\n",
    "\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "def time_decay(t1, t2, alpha=0.1):\n",
    "    \"\"\"\n",
    "    A time-based decay function that decreases the impact of ratings farther in time.\n",
    "    t1, t2: Datetime objects representing the timestamps of the ratings.\n",
    "    alpha: A decay parameter that determines the rate of decay.\n",
    "    \"\"\"\n",
    "    time_diff = abs((t1 - t2).days/10000)\n",
    "    return math.exp(-alpha * time_diff)\n",
    "\n",
    "def convert_to_datetime(date_string):\n",
    "    \"\"\"\n",
    "    Convert the date string in the format 'YYYY-MM-DD' to a datetime object.\n",
    "    \"\"\"\n",
    "    return datetime.strptime(date_string, '%Y-%m-%d')\n",
    "    \n",
    "def predictRating_Jaccard_with_decay(user, item, target_time):\n",
    "    if item not in usersPerItem:\n",
    "        return ratingMean\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        k = d['product_id']\n",
    "        if k != item:\n",
    "            ratings.append(d['star_rating'] - itemsAverages[k])\n",
    "            exp_rate = time_decay(convert_to_datetime(d['review_date']),convert_to_datetime(target_time))  # assuming 'review_date' is in Unix timestamp format\n",
    "            similarities.append(Jaccard(usersPerItem[item], usersPerItem[k]) * exp_rate)\n",
    "    if sum(similarities) != 0:\n",
    "        weighted_ratings = [x * y for x, y in zip(ratings, similarities)]\n",
    "        return itemsAverages[item] + sum(weighted_ratings) / sum(similarities)\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        return itemsAverages[item]\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x - y) ** 2 for x, y in zip(predictions, labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "labels = [d['star_rating'] for d in dataTest]\n",
    "time_decayed_predictions = [predictRating_Jaccard_with_decay(d['customer_id'], d['product_id'], d['review_date']) for d in dataTest]\n",
    "\n",
    "mse_time_decay = MSE(time_decayed_predictions, labels)\n",
    "print(\"Mean Squared Error (MSE) on the test set with time decay:\", mse_time_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "addeab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes below are used for answer.txt generation\n",
    "\n",
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N\n",
    "\n",
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86aaadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1\n",
    "\n",
    "answers['Q1'] = [0.16130237168160533, 0.1607838024608832]\n",
    "\n",
    "assertFloatList(answers['Q1'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41f14776",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2\n",
    "\n",
    "answers['Q2'] = [0.14214313781636712, 0.14301810257164882]\n",
    "\n",
    "assertFloatList(answers['Q2'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b124061",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3\n",
    "\n",
    "answers['Q3'] = [0.1, 0.1432824729179234, 0.14544069028123996]\n",
    "\n",
    "assertFloatList(answers['Q3'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d58c7d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 4\n",
    "\n",
    "answers['Q4'] = [0.3139492057092712, 0.16109632033831978, 0.14658340274812243]\n",
    "\n",
    "assertFloatList(answers['Q4'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a871ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 5\n",
    "\n",
    "answers['Q5'] = ms\n",
    "\n",
    "assertFloatList([m[0] for m in ms], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64dea730",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 6\n",
    "\n",
    "answers['Q6'] = 1.7165666373341593\n",
    "\n",
    "assertFloat(answers['Q6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "654018e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7\n",
    "\n",
    "answers['Q7'] = [\"Add time decay with decay rate 0.1\", 1.716566096661067]\n",
    "\n",
    "assertFloat(answers['Q7'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcc1cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw2.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908e905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
