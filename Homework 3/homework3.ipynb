{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c379253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the baseline model on the validation set: 0.6842\n"
     ]
    }
   ],
   "source": [
    "# Task 1: construct negative samples for the validation set\n",
    "\n",
    "\"\"\"\n",
    "sample a negative entry by randomly choosing a game that user hasn’t played for each entry (user,game) in the validation set.\n",
    "evaluate the performance (accuracy) of the baseline model on the validation set you have built.\n",
    "\"\"\"\n",
    "\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Load data with functions from stub\n",
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "\n",
    "def readJSON(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        try:\n",
    "            g = d['gameID']\n",
    "        except Exception as e:  # In case the 'gameID' attribute is missing.\n",
    "            g = None\n",
    "        yield u, g, d\n",
    "\n",
    "allHours = []\n",
    "for l in readJSON(\"train.json.gz\"):\n",
    "    allHours.append(l)\n",
    "\n",
    "# Split the data into training and validation sets.\n",
    "hoursTrain = allHours[:165000]\n",
    "hoursValid = allHours[165000:]\n",
    "\n",
    "# Data structures for the would-play baseline (from baseline.py)\n",
    "gameCount = defaultdict(int)  # Dictionary to keep track of how many times each game was played.\n",
    "totalPlayed = 0  # Total number of games played.\n",
    "\n",
    "# Populate gameCount dictionary and totalPlayed count.\n",
    "for user, game, _ in readJSON(\"train.json.gz\"):\n",
    "    gameCount[game] += 1\n",
    "    totalPlayed += 1\n",
    "\n",
    "# Sort games by popularity (number of times played).\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "# Find the top half of the most popular games to create a return set.\n",
    "return1 = set()  # Set of games that are most popular.\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    # Cover at least half of the total plays.\n",
    "    if count > totalPlayed/2:\n",
    "        break\n",
    "\n",
    "# Find out which games a user hasn't played yet.\n",
    "userPlayedGames = defaultdict(set)\n",
    "for user, game, _ in readJSON(\"train.json.gz\"):\n",
    "    userPlayedGames[user].add(game)\n",
    "\n",
    "# Create validation set which includes both positive and negative samples.\n",
    "allGames = list(gameCount.keys())  # All distinct games in our dataset.\n",
    "validationSet = []\n",
    "\n",
    "# Generate negative samples for the valid set.\n",
    "for user, game, _ in hoursValid:\n",
    "    negativeGame = random.choice(allGames)\n",
    "    while negativeGame in userPlayedGames[user]:\n",
    "        negativeGame = random.choice(allGames)\n",
    "    validationSet.append((user, game, 1))        # Positive example: The actual game played by the user\n",
    "    validationSet.append((user, negativeGame, 0)) # Negative example: A randomly chosen game (not played by the user)\n",
    "\n",
    "# Evaluate the accuracy of the baseline model on this validation set.\n",
    "correctPredictions = 0\n",
    "\n",
    "for user, game, actual in validationSet:\n",
    "    prediction = 1 if game in return1 else 0\n",
    "    if prediction == actual:\n",
    "        correctPredictions += 1\n",
    "\n",
    "# Calculate accuracy.\n",
    "accuracy = correctPredictions / len(validationSet)\n",
    "print(f\"Accuracy of the baseline model on the validation set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb9f4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.68\n",
      "Accuracy on validation set using best threshold: 0.7056\n"
     ]
    }
   ],
   "source": [
    "# Task 2: improve the model performance with a better threshold\n",
    "\n",
    "\"\"\"\n",
    "find a better threshold and report its performance on your validation set.\n",
    "\"\"\"\n",
    "\n",
    "def get_most_popular_games(threshold, mostPopular, totalPlayed):\n",
    "    \"\"\"Return a set of games considered 'popular' for a given threshold.\"\"\"\n",
    "    popular_games = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        popular_games.add(i)\n",
    "        if count > totalPlayed * threshold:\n",
    "            break\n",
    "    return popular_games\n",
    "\n",
    "def evaluate_popularity_threshold(threshold, mostPopular, totalPlayed, validationSet):\n",
    "    \"\"\"Evaluate and return the accuracy of a given popularity threshold on the validation set.\"\"\"\n",
    "    popular_games = get_most_popular_games(threshold, mostPopular, totalPlayed)\n",
    "    correct_predictions = sum(1 for user, game, actual in validationSet if (game in popular_games) == bool(actual))\n",
    "    return correct_predictions / len(validationSet)\n",
    "\n",
    "# Search for the best threshold by iterating over a range of possible values.\n",
    "best_accuracy = 0\n",
    "best_threshold = 0\n",
    "for threshold in [i*0.01 for i in range(101)]:  # Iterating from 0 to 1 with a step of 0.01\n",
    "    accuracy = evaluate_popularity_threshold(threshold, mostPopular, totalPlayed, validationSet)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.2f}\")\n",
    "print(f\"Accuracy on validation set using best threshold: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22a7ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold for Jaccard similarity: 0.03\n",
      "Accuracy on validation set using best threshold: 0.6688\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Jaccard similarity-based threshold\n",
    "\n",
    "\"\"\"\n",
    "given a pair (u, g) in the validation set, consider all training items.\n",
    "compute the Jaccard similarity.\n",
    "predict as ‘played’ if the maximum of these Jaccard similarities exceeds a threshold. \n",
    "report the performance on validation set.\n",
    "\"\"\"\n",
    "\n",
    "# Create a dictionary with games as keys and sets of users who played them as values.\n",
    "gameUsers = defaultdict(set)\n",
    "for user, game, _ in hoursTrain:\n",
    "    gameUsers[game].add(user)\n",
    "\n",
    "# Calculate Jaccard similarity between two sets.\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Precompute Jaccard similarities for all game pairs.\n",
    "game_pair_similarities = defaultdict(dict)\n",
    "all_games = list(gameUsers.keys())\n",
    "\n",
    "for i, game1 in enumerate(all_games):\n",
    "    for j, game2 in enumerate(all_games):\n",
    "        if j <= i:\n",
    "            continue  # No need to compute similarity twice for the same pair\n",
    "        similarity = jaccard_similarity(gameUsers[game1], gameUsers[game2])\n",
    "        game_pair_similarities[game1][game2] = similarity\n",
    "        game_pair_similarities[game2][game1] = similarity\n",
    "\n",
    "def predict_played_optimized(u, g, threshold):\n",
    "    max_similarity = 0\n",
    "    for game_prime in userPlayedGames[u]:\n",
    "        if game_prime in game_pair_similarities[g]:\n",
    "            similarity = game_pair_similarities[g][game_prime]\n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "    return 1 if max_similarity > threshold else 0\n",
    "\n",
    "# Now, search for the best threshold using the optimized function.\n",
    "best_threshold = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for threshold in [i*0.01 for i in range(101)]:\n",
    "    correct_predictions = sum(1 for user, game, actual in validationSet if predict_played_optimized(user, game, threshold) == actual)\n",
    "    accuracy = correct_predictions / len(validationSet)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold for Jaccard similarity: {best_threshold:.2f}\")\n",
    "print(f\"Accuracy on validation set using best threshold: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edce48f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the combined model on the validation set: 0.6692\n"
     ]
    }
   ],
   "source": [
    "# Task 4: incorporate Jaccard-based and popularity based threshold\n",
    "\n",
    "\"\"\"\n",
    "incorporate both a Jaccard-based threshold and a popularity based threshold. \n",
    "report the performance on your validation set\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare the features and labels.\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for user, game, actual in validationSet:\n",
    "    # Feature 1: Jaccard similarity\n",
    "    max_similarity = 0\n",
    "    for game_prime in userPlayedGames[user]:\n",
    "        if game_prime in game_pair_similarities[game]:\n",
    "            similarity = game_pair_similarities[game][game_prime]\n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "    # Feature 2: Popularity\n",
    "    is_popular = int(game in get_most_popular_games(best_threshold, mostPopular, totalPlayed))\n",
    "    X.append([max_similarity, is_popular])\n",
    "    y.append(actual)\n",
    "\n",
    "# Split X and y into training and test datasets (optional, but typically a good idea).\n",
    "# Note: You might want to have a separate test set to evaluate this.\n",
    "\n",
    "# Train a logistic regression model.\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict on the validation set.\n",
    "y_pred = clf.predict(X)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "print(f\"Accuracy of the combined model on the validation set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9f9498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5:\n",
    "\n",
    "\"\"\"\n",
    "use the files ‘pairs Played.txt’ to find the reviewerID/itemID pairs about which we have to make predictions. \n",
    "use that data, run the above model and upload your solution to the Assignment 1 gradescope.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Open the test file and prediction file.\n",
    "predictions = open(\"HWpredictions_Played.csv\", 'w')\n",
    "\n",
    "for l in open(\"pairs_Played.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # Write the header to the output file.\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, g = l.strip().split(',')\n",
    "    \n",
    "    # Extract features for the given (u, g) pair.\n",
    "    # Feature 1: Jaccard similarity.\n",
    "    max_similarity = 0\n",
    "    if u in userPlayedGames:  # Make sure the user exists in the training data.\n",
    "        for game_prime in userPlayedGames[u]:\n",
    "            if game_prime in game_pair_similarities[game]:\n",
    "                similarity = game_pair_similarities[game][game_prime]\n",
    "                max_similarity = max(max_similarity, similarity)\n",
    "    \n",
    "    # Feature 2: Popularity.\n",
    "    is_popular = int(game in get_most_popular_games(best_threshold, mostPopular, totalPlayed))\n",
    "    \n",
    "    # Make a prediction using the logistic regression model.\n",
    "    pred = clf.predict([[max_similarity, is_popular]])[0]  # clf is the trained logistic regression model.\n",
    "    \n",
    "    # Write the prediction to the output file.\n",
    "    predictions.write(u + ',' + g + ',' + str(pred) + '\\n')\n",
    "\n",
    "# Close the prediction file.\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e432437f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca39475f8d94999837455231aae7560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 3.0120\n"
     ]
    }
   ],
   "source": [
    "# Task 6: time played predictor\n",
    "\n",
    "\"\"\"\n",
    "fit a predictor by fitting the mean and the two bias terms.\n",
    "use a regularization parameter of λ = 1. \n",
    "report the MSE on the validation set.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Calculate global average\n",
    "trainHours = [r[2]['hours_transformed'] for r in hoursTrain]\n",
    "globalAverage = sum(trainHours) / len(trainHours)\n",
    "\n",
    "# Initialize dictionaries\n",
    "hoursPerUser = defaultdict(float)\n",
    "hoursPerItem = defaultdict(float)\n",
    "gamesPerUser = defaultdict(int)\n",
    "usersPerItem = defaultdict(int)\n",
    "betaU = defaultdict(float)\n",
    "betaI = defaultdict(float)\n",
    "\n",
    "# Populate the dictionaries\n",
    "for user, game, d in hoursTrain:\n",
    "    hoursPerUser[user] += d['hours_transformed']\n",
    "    hoursPerItem[game] += d['hours_transformed']\n",
    "    gamesPerUser[user] += 1\n",
    "    usersPerItem[game] += 1\n",
    "\n",
    "alpha = globalAverage  # Initialize alpha\n",
    "\n",
    "# Regularization parameter\n",
    "lamb = 1\n",
    "\n",
    "def iterate(lamb):\n",
    "    newAlpha = 0\n",
    "    for user, game, d in hoursTrain:\n",
    "        newAlpha += d['hours_transformed'] - (betaU[user] + betaI[game])\n",
    "    alpha = newAlpha / len(hoursTrain)\n",
    "\n",
    "    for user in betaU:\n",
    "        newBetaU = 0\n",
    "        for r in hoursTrain:\n",
    "            if r[0] == user:\n",
    "                _, game, d = r\n",
    "                newBetaU += d['hours_transformed'] - (alpha + betaI[game])\n",
    "        betaU[user] = newBetaU / (lamb + gamesPerUser[user])\n",
    "\n",
    "    for game in betaI:\n",
    "        newBetaI = 0\n",
    "        for r in hoursTrain:\n",
    "            if r[1] == game:\n",
    "                user, _, d = r\n",
    "                newBetaI += d['hours_transformed'] - (alpha + betaU[user])\n",
    "        betaI[game] = newBetaI / (lamb + usersPerItem[game])\n",
    "\n",
    "from tqdm.notebook import tqdm # for visulization\n",
    "\n",
    "# Perform iterations to refine alpha, betaU, and betaI\n",
    "for _ in tqdm(range(10)):  # Use tqdm here for progress bar\n",
    "    iterate(lamb)\n",
    "\n",
    "# Calculate MSE on the validation set\n",
    "mse_sum = 0\n",
    "for user, game, d in hoursValid:\n",
    "    prediction = alpha + betaU[user] + betaI[game]\n",
    "    mse_sum += (d['hours_transformed'] - prediction) ** 2\n",
    "\n",
    "mse = mse_sum / len(hoursValid)\n",
    "print(f\"MSE on validation set: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c34fecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User with the largest beta value: u60898505 with value 5.8266\n",
      "User with the smallest beta value: u13037838 with value -3.0066\n",
      "Game with the largest beta value: g17604638 with value 5.3609\n",
      "Game with the smallest beta value: g84397720 with value -2.9435\n"
     ]
    }
   ],
   "source": [
    "# Task 7: report the user and game IDs\n",
    "\n",
    "\"\"\"\n",
    "report the user and game IDs that have the largest and smallest values of β.\n",
    "\"\"\"\n",
    "\n",
    "# Find the user with the largest and smallest beta values\n",
    "max_betaU_user = max(betaU, key=betaU.get)\n",
    "min_betaU_user = min(betaU, key=betaU.get)\n",
    "\n",
    "# Find the game with the largest and smallest beta values\n",
    "max_betaI_game = max(betaI, key=betaI.get)\n",
    "min_betaI_game = min(betaI, key=betaI.get)\n",
    "\n",
    "print(f\"User with the largest beta value: {max_betaU_user} with value {betaU[max_betaU_user]:.4f}\")\n",
    "print(f\"User with the smallest beta value: {min_betaU_user} with value {betaU[min_betaU_user]:.4f}\")\n",
    "print(f\"Game with the largest beta value: {max_betaI_game} with value {betaI[max_betaI_game]:.4f}\")\n",
    "print(f\"Game with the smallest beta value: {min_betaI_game} with value {betaI[min_betaI_game]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a93bfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------lambda=0.001-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcdbbd7591f404eadcd4d1a9a6e932d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.001, MSE: 3.0217\n",
      "-------------lambda=0.01-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9084266aa8e646d1973ec3a8b1db65d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.01, MSE: 3.0215\n",
      "-------------lambda=0.1-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b401d0ba54974b82a86a872786cbaf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.1, MSE: 3.0198\n",
      "-------------lambda=1-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63b416db65941e580f5b891770f1158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 1, MSE: 3.0120\n",
      "-------------lambda=10-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6cdd60e22f40ffbb774862f739c600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 10, MSE: 3.1446\n",
      "-------------lambda=100-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70d8f7b8426455f87977e06db201ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 100, MSE: 3.5667\n",
      "Best lambda: 1, with MSE: 3.0120\n"
     ]
    }
   ],
   "source": [
    "# Task 8: find a better value of λ\n",
    "\n",
    "\"\"\"\n",
    "find a better value of λ using your validation set. \n",
    "report the value you chose, its MSE.\n",
    "\"\"\"\n",
    "\n",
    "def mse(validationSet, alpha, betaU, betaI):\n",
    "    \"\"\"Calculate the mean squared error on the validation set.\"\"\"\n",
    "    errors = [(r[2]['hours_transformed'] - (alpha + betaU[r[0]] + betaI[r[1]])) ** 2 for r in validationSet]\n",
    "    return sum(errors) / len(errors)\n",
    "\n",
    "# Initialize variables to store the best lambda and its corresponding MSE\n",
    "best_lambda = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "# Iterate over a range of lambda values\n",
    "for lamb in [0.001, 0.01, 0.1, 1, 10, 100]:  # Adjust this range and values as necessary\n",
    "    \n",
    "    print(f'-------------lambda={lamb}-------------')\n",
    "    # Initialize beta values for this lambda\n",
    "    for u in hoursPerUser:\n",
    "        betaU[u] = 0\n",
    "\n",
    "    for g in hoursPerItem:\n",
    "        betaI[g] = 0\n",
    "\n",
    "    alpha = globalAverage  # Reset alpha\n",
    "\n",
    "    # Perform the iterations\n",
    "    for _ in tqdm(range(10)):  # use 10 here for efficiency\n",
    "        iterate(lamb)\n",
    "\n",
    "    # Calculate MSE for this lambda\n",
    "    current_mse = mse(hoursValid, alpha, betaU, betaI)\n",
    "    print(f\"Lambda: {lamb}, MSE: {current_mse:.4f}\")\n",
    "\n",
    "    # Update best lambda and MSE if current MSE is lower\n",
    "    if current_mse < best_mse:\n",
    "        best_mse = current_mse\n",
    "        best_lambda = lamb\n",
    "\n",
    "# Print the best lambda and its MSE\n",
    "print(f\"Best lambda: {best_lambda}, with MSE: {best_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6a6d371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------lambda=0.25-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1488a36e3594356bcdb94a3b0d0b0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.25, MSE: 3.0174\n",
      "-------------lambda=0.5-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86bb9ef4c7c435e84a2b0e3b9020ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.5, MSE: 3.0145\n",
      "-------------lambda=1.5-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6370ba59fd42138dd3978c6eddb754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 1.5, MSE: 3.0130\n",
      "-------------lambda=2-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1ee9b620d44bec91234480cdd04b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 2, MSE: 3.0165\n",
      "-------------lambda=2.5-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153755980ce44d98b5bca64643227a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 2.5, MSE: 3.0219\n",
      "-------------lambda=3-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271e912151ea48abb16a1e651375355b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 3, MSE: 3.0287\n",
      "-------------lambda=3.5-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1869b27c5cf4ea7b1deb094db48aae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 3.5, MSE: 3.0363\n",
      "-------------lambda=4-------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52832204e52419c8eecddfd5d42b60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 4, MSE: 3.0446\n",
      "Best lambda: 1.5, with MSE: 3.0130\n"
     ]
    }
   ],
   "source": [
    "# Complimentary Experiment for other lambdas as 1 is the best in previous setting\n",
    "def mse(validationSet, alpha, betaU, betaI):\n",
    "    \"\"\"Calculate the mean squared error on the validation set.\"\"\"\n",
    "    errors = [(r[2]['hours_transformed'] - (alpha + betaU[r[0]] + betaI[r[1]])) ** 2 for r in validationSet]\n",
    "    return sum(errors) / len(errors)\n",
    "\n",
    "# Initialize variables to store the best lambda and its corresponding MSE\n",
    "best_lambda = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "# Iterate over a range of lambda values\n",
    "for lamb in [0.25, 0.5, 1.5, 2, 2.5, 3, 3.5, 4]:  # Adjust this range and values as necessary\n",
    "    \n",
    "    print(f'-------------lambda={lamb}-------------')\n",
    "    # Initialize beta values for this lambda\n",
    "    for u in hoursPerUser:\n",
    "        betaU[u] = 0\n",
    "\n",
    "    for g in hoursPerItem:\n",
    "        betaI[g] = 0\n",
    "\n",
    "    alpha = globalAverage  # Reset alpha\n",
    "\n",
    "    # Perform the iterations\n",
    "    for _ in tqdm(range(10)):  # use 10 here for efficiency\n",
    "        iterate(lamb)\n",
    "\n",
    "    # Calculate MSE for this lambda\n",
    "    current_mse = mse(hoursValid, alpha, betaU, betaI)\n",
    "    print(f\"Lambda: {lamb}, MSE: {current_mse:.4f}\")\n",
    "\n",
    "    # Update best lambda and MSE if current MSE is lower\n",
    "    if current_mse < best_mse:\n",
    "        best_mse = current_mse\n",
    "        best_lambda = lamb\n",
    "\n",
    "# Print the best lambda and its MSE\n",
    "print(f\"Best lambda: {best_lambda}, with MSE: {best_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "661b1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N\n",
    "    \n",
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4dc1b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1'] = 0.6842\n",
    "assertFloat(answers['Q1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49f79cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = [0.68,0.7056]\n",
    "assertFloatList(answers['Q2'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0c7a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = 0.6688\n",
    "assertFloat(answers['Q3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6dcf523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4'] = 0.6692\n",
    "assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53b98a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = \"I confirm that I have uploaded an assignment submission to gradescope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09fb7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6'] = 3.0120\n",
    "assertFloat(answers['Q6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e14d7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = [5.8266, -3.0066, 5.3609, -2.9435]\n",
    "assertFloatList(answers['Q7'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2c82b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q8'] = (1.5, 1.0115)\n",
    "assertFloatList(answers['Q8'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16e9713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw3.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a7b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
